############################################################################################
## DS scraping assignment: Is hard news a downer? Scraping and analyzing Twitter data
## Kathryn Zickuhr
## Due February 23, 2015
############################################################################################

# This portion scrapes 5000 tweets from news accounts on Twitter (500 each from 10 accounts),
# based on Pablo Barbera's workshop, "Collecting and Analyzing Social Media Data with R".

# "1. Using the Twitter API, download at least 5000 tweets from 10 media sources (or 500
# tweets/media source). One twitter handle = one media source. You get to choose what media
# sources you want to look at." 

setwd("/Users/Katja/DS/scraping/twitter")
getwd()

## INSTALLING PACKAGES THAT WE WILL USE TODAY
install.packages("ROAuth", "twitteR", "streamR", "RJSONIO", "ggplot2", "stringr", "tm", "RCurl")

#####################################
### CREATING YOUR OWN OAUTH TOKEN ###
#####################################

## Step 1: go to apps.twitter.com and sign in
## Step 2: click on "Create New App"
## Step 3: fill name, description, and website (it can be anything, even google.com)
##			(make sure you leave 'Callback URL' empty)
## Step 4: Agree to user conditions
## Step 5: copy consumer key and consumer secret and paste below

library(ROAuth)
requestURL <- "https://api.twitter.com/oauth/request_token"
accessURL <- "https://api.twitter.com/oauth/access_token"
authURL <- "https://api.twitter.com/oauth/authorize"
consumerKey <- "r6QulDIyJjGS8ToSfrbHiyF0Q"
consumerSecret <- "eImLpiUIowkxcnySmqJG9WyHk45NxiRrVSZvmkZGl2m3tWErVb"

my_oauth <- OAuthFactory$new(consumerKey=consumerKey,
  consumerSecret=consumerSecret, requestURL=requestURL,
  accessURL=accessURL, authURL=authURL)

## run this line and go to the URL that appears on screen
my_oauth$handshake(cainfo = system.file("CurlSSL", "cacert.pem", package = "RCurl"))

## now save the oauth token for use in future sessions with twitteR or streamR
save(my_oauth, file="oauth_token.Rdata")

### Twitter's access token and access secret can be found in 'Keys and Access Tokens'
### tab in apps.twitter.com

accessToken = '24484875-YKgY4EBOHvQXli7cftX277oH6SsPTxiiisxjClGlk'
accessSecret = '5enVsckD87iEyARq6V92vmMEgAgSFNWN7YlqbYGxwIl2d'

## testing that it works
library(twitteR)
setup_twitter_oauth(consumer_key=consumerKey, consumer_secret=consumerSecret,
	access_token=accessToken, access_secret=accessSecret)
searchTwitter('obama', n=1)

## from a Windows machine:
# searchTwitter("obama", cainfo = system.file("CurlSSL", "cacert.pem", package = "RCurl"))

###############################################
### DOWNLOADING RECENT TWEETS FROM 10 USERS ###
###############################################

newsAccounts <- c("GovBeat",
                  "postlocal",
                  "postpolitics",
                  "postworldnews",
                  "washpostbiz",
                  "wpinvestigates",
                  "WSJbreakingnews",
                  "WSJbusiness",
                  "WSJmarkets",
                  "WSJPolitics")
for (newsAccount in newsAccounts) {

## Using Barbera's function to store the raw JSON data
source("functions.r")

getTimeline(filename=paste0("tweets_",newsAccount,".json"),
    n=500, oauth=my_oauth, trim_user="false", newsAccount)
  
# Read it with the 'parseTweets' function in the streamR package
library(streamR)
tweets <- parseTweets(paste0("tweets_",newsAccount,".json"))

}

##############################################################################################

# This portion conducts basic sentiment anlysis on the 5000 media tweets previously gathered,
# based on Pablo Barbera's workshop, "Collecting and Analyzing Social Media Data with R".

# Loading libraries we will use
library(streamR)
library(ggplot2)
library(grid)

#####################################
### SENTIMENT ANALYSIS	          ###
#####################################

fileNames <- c("tweets_GovBeat.json",
                "tweets_postlocal.json",
                "tweets_postpolitics.json",
                "tweets_postworldnews.json",
                "tweets_washpostbiz.json",
                "tweets_wpinvestigates.json",
                "tweets_WSJbreakingnews.json",
                "tweets_WSJbusiness.json",
                "tweets_WSJmarkets.json",
                "tweets_WSJPolitics.json")
for (fileName in fileNames) {
  
# Loading tweets we will use
tweets <- parseTweets(fileName)

# loading lexicon of positive and negative words (from Neal Caren)
lexicon <- read.csv("lexicon.csv", stringsAsFactors=F)
pos.words <- lexicon$word[lexicon$polarity=="positive"]
neg.words <- lexicon$word[lexicon$polarity=="negative"]

# a look at a random sample of positive and negative words
sample(pos.words, 10)
sample(neg.words, 10)

# function to clean the text
clean_tweets <- function(text){
    # loading required packages
    lapply(c("tm", "Rstem", "stringr"), require, c=T, q=T)
    # avoid encoding issues by dropping non-unicode characters
    utf8text <- iconv(text, to='UTF-8-MAC', sub = "byte")
    # remove punctuation and convert to lower case
    words <- removePunctuation(utf8text)
    words <- tolower(words)
    # spliting in words
    words <- str_split(words, " ")
    return(words)
}

# now we clean the text
tweets$text[1]
tweets$text[7]

text <- clean_tweets(tweets$text)
text[[1]]
text[[7]]

# a function to classify individual tweets
classify <- function(words, pos.words, neg.words){
    # count number of positive and negative word matches
    pos.matches <- sum(words %in% pos.words)
    neg.matches <- sum(words %in% neg.words)
    return(pos.matches - neg.matches)
}

# this is how we would apply it
classify(text[[1]], pos.words, neg.words)
classify(text[[7]], pos.words, neg.words)

# but we want to aggregate over many tweets...
classifier <- function(text, pos.words, neg.words){
    # classifier
    scores <- unlist(lapply(text, classify, pos.words, neg.words))
    n <- length(scores)
    positive <- as.integer(length(which(scores>0))/n*100)
    negative <- as.integer(length(which(scores<0))/n*100)
    neutral <- 100 - positive - negative
    cat("The sentiment of",fileName,"is:", positive,"% positive,",
        negative,"% negative,", neutral,"% neutral","(out of", n, "tweets.)\n")
}

# applying classifier function
classifier(text, pos.words, neg.words)

}

# Combining tweets from separate accounts into one file and conducting the analysis:

setwd("/Users/Katja/DS/scraping/twitter")
getwd()

# Loading libraries we will use
library(streamR)
library(ggplot2)
library(grid)

tweets <- parseTweets("merged_tweets.json")

# loading lexicon of positive and negative words (from Neal Caren)
lexicon <- read.csv("lexicon.csv", stringsAsFactors=F)
pos.words <- lexicon$word[lexicon$polarity=="positive"]
neg.words <- lexicon$word[lexicon$polarity=="negative"]

sample(pos.words, 10)
sample(neg.words, 10)

# function to clean the text
clean_tweets <- function(text){
  # loading required packages
  lapply(c("tm", "Rstem", "stringr"), require, c=T, q=T)
  # avoid encoding issues by dropping non-unicode characters
  utf8text <- iconv(text, to='UTF-8-MAC', sub = "byte")
  # remove punctuation and convert to lower case
  words <- removePunctuation(utf8text)
  words <- tolower(words)
  # spliting in words
  words <- str_split(words, " ")
  return(words)
}

# clean the text
tweets$text[1]
tweets$text[7]

text <- clean_tweets(tweets$text)
text[[1]]
text[[7]]

# a function to classify individual tweets
classify <- function(words, pos.words, neg.words){
  # count number of positive and negative word matches
  pos.matches <- sum(words %in% pos.words)
  neg.matches <- sum(words %in% neg.words)
  return(pos.matches - neg.matches)
}

classify(text[[1]], pos.words, neg.words)
classify(text[[7]], pos.words, neg.words)

classifier <- function(text, pos.words, neg.words){
  # classifier
  scores <- unlist(lapply(text, classify, pos.words, neg.words))
  n <- length(scores)
  positive <- as.integer(length(which(scores>0))/n*100)
  negative <- as.integer(length(which(scores<0))/n*100)
  neutral <- 100 - positive - negative
  cat("The sentiment of the combined tweets is:", positive,"% positive,",
      negative,"% negative,", neutral,"% neutral","(out of", n, "tweets.)\n")
}

# applying classifier function
classifier(text, pos.words, neg.words)
