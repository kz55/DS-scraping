############################################################################################
## DS scraping assignment: Is hard news a downer? Scraping and analyzing Twitter data
## Kathryn Zickuhr
## Due February 23, 2015
############################################################################################

# This portion scrapes 5000 tweets from news accounts on Twitter (500 each from 10
# accounts), based on Pablo Barbera's workshop, "Collecting and Analyzing Social Media
# Data with R". After saving the results to a CSV file, we produce two charts illustrating
# the data (showing the results for individual accounts and for the sample as a whole.)

# "1. Using the Twitter API, download at least 5000 tweets from 10 media sources (or 500
# tweets/media source). One twitter handle = one media source. You get to choose what
# media sources you want to look at." 

setwd("~DS/scraping/twitter")
getwd()

## INSTALLING PACKAGES THAT WE WILL USE TODAY
install.packages("ROAuth", "twitteR", "streamR", "RJSONIO", "ggplot2", "stringr", "tm", "RCurl")

#####################################
### CREATING YOUR OWN OAUTH TOKEN ###
#####################################

## Step 1: Go to apps.twitter.com and sign in
## Step 2: Click on "Create New App"
## Step 3: Fill name, description, and website, leave 'Callback URL' empty.
## Step 4: Agree to user conditions
## Step 5: Copy consumer key and consumer secret and paste below

library(ROAuth)
requestURL <- "https://api.twitter.com/oauth/request_token"
accessURL <- "https://api.twitter.com/oauth/access_token"
authURL <- "https://api.twitter.com/oauth/authorize"
consumerKey <- "r6QulDIyJjGS8ToSfrbHiyF0Q"
consumerSecret <- "eImLpiUIowkxcnySmqJG9WyHk45NxiRrVSZvmkZGl2m3tWErVb"

my_oauth <- OAuthFactory$new(consumerKey=consumerKey,
  consumerSecret=consumerSecret, requestURL=requestURL,
  accessURL=accessURL, authURL=authURL)

## run this line and go to the URL that appears on screen
my_oauth$handshake(cainfo = system.file("CurlSSL", "cacert.pem", package = "RCurl"))

## now save the oauth token for use in future sessions with twitteR or streamR
save(my_oauth, file="oauth_token.Rdata")

### Twitter's access token and access secret can be found in 'Keys and Access Tokens'
### tab in apps.twitter.com

accessToken = '24484875-YKgY4EBOHvQXli7cftX277oH6SsPTxiiisxjClGlk'
accessSecret = '5enVsckD87iEyARq6V92vmMEgAgSFNWN7YlqbYGxwIl2d'

## testing that it works
library(twitteR)
setup_twitter_oauth(consumer_key=consumerKey, consumer_secret=consumerSecret,
	access_token=accessToken, access_secret=accessSecret)
searchTwitter('obama', n=1)

## from a Windows machine:
# searchTwitter("obama", cainfo = system.file("CurlSSL", "cacert.pem", package = "RCurl"))

########################################################
### DOWNLOADING RECENT TWEETS FROM 10 MEDIA ACCOUNTS ###
########################################################

newsAccounts <- c("GovBeat",
                  "postlocal",
                  "postpolitics",
                  "postworldnews",
                  "washpostbiz",
                  "wpinvestigates",
                  "WSJbreakingnews",
                  "WSJbusiness",
                  "WSJmarkets",
                  "WSJPolitics")
for (newsAccount in newsAccounts) {

## Using Barbera's function to store the raw JSON data
source("functions.r")

getTimeline(filename=paste0("tweets_",newsAccount,".json"),
    n=500, oauth=my_oauth, trim_user="false", newsAccount)
  
# Read it with the 'parseTweets' function in the streamR package
library(streamR)
tweets <- parseTweets(paste0("tweets_",newsAccount,".json"))

}

##########################################################################################

# This portion conducts basic sentiment analysis on the 5000 media tweets previously gathered,
# based on Pablo Barbera's workshop, "Collecting and Analyzing Social Media Data with R".

# Loading libraries we will use
library(streamR)
library(ggplot2)
library(grid)

###########################
### SENTIMENT ANALYSIS  ###
###########################

fileNames <- c("tweets_GovBeat.json",
                "tweets_postlocal.json",
                "tweets_postpolitics.json",
                "tweets_postworldnews.json",
                "tweets_washpostbiz.json",
                "tweets_wpinvestigates.json",
                "tweets_WSJbreakingnews.json",
                "tweets_WSJbusiness.json",
                "tweets_WSJmarkets.json",
                "tweets_WSJPolitics.json")
for (fileName in fileNames) {
  
# Loading tweets we will use
tweets <- parseTweets(fileName)

# loading lexicon of positive and negative words (from Neal Caren)
lexicon <- read.csv("lexicon.csv", stringsAsFactors=F)
pos.words <- lexicon$word[lexicon$polarity=="positive"]
neg.words <- lexicon$word[lexicon$polarity=="negative"]

# a look at a random sample of positive and negative words
sample(pos.words, 10)
sample(neg.words, 10)

# function to clean the text
clean_tweets <- function(text){
    # loading required packages
    lapply(c("tm", "Rstem", "stringr"), require, c=T, q=T)
    # avoid encoding issues by dropping non-unicode characters
    utf8text <- iconv(text, to='UTF-8-MAC', sub = "byte")
    # remove punctuation and convert to lower case
    words <- removePunctuation(utf8text)
    words <- tolower(words)
    # spliting in words
    words <- str_split(words, " ")
    return(words)
}

# Clean the text
tweets$text[1]
tweets$text[7]

text <- clean_tweets(tweets$text)
text[[1]]
text[[7]]

# funciton to classify individual tweets
classify <- function(words, pos.words, neg.words){
    # count number of positive and negative word matches
    pos.matches <- sum(words %in% pos.words)
    neg.matches <- sum(words %in% neg.words)
    return(pos.matches - neg.matches)
}

# How we would apply it
classify(text[[1]], pos.words, neg.words)
classify(text[[7]], pos.words, neg.words)

# Aggregate classifier over many tweets
classifier <- function(text, pos.words, neg.words){
    # classifier
    scores <- unlist(lapply(text, classify, pos.words, neg.words))
    n <- length(scores)
    positive <- as.integer(length(which(scores>0))/n*100)
    negative <- as.integer(length(which(scores<0))/n*100)
    neutral <- 100 - positive - negative
    cat("The sentiment of",fileName,"is:", positive,"% positive,",
        negative,"% negative,", neutral,"% neutral","(out of", n, "tweets.)\n")
}

# Applying classifier function
classifier(text, pos.words, neg.words)

}

# We now have the results for each individual news account.

### Combine tweets from separate accounts into one file to analyze as a whole:

tweets <- parseTweets("merged_tweets.json")

# loading lexicon of positive and negative words (from Neal Caren)
lexicon <- read.csv("lexicon.csv", stringsAsFactors=F)
pos.words <- lexicon$word[lexicon$polarity=="positive"]
neg.words <- lexicon$word[lexicon$polarity=="negative"]

sample(pos.words, 10)
sample(neg.words, 10)

# function to clean the text
clean_tweets <- function(text){
  # loading required packages
  lapply(c("tm", "Rstem", "stringr"), require, c=T, q=T)
  # avoid encoding issues by dropping non-unicode characters
  utf8text <- iconv(text, to='UTF-8-MAC', sub = "byte")
  # remove punctuation and convert to lower case
  words <- removePunctuation(utf8text)
  words <- tolower(words)
  # spliting in words
  words <- str_split(words, " ")
  return(words)
}

# clean the text
tweets$text[1]
tweets$text[7]

text <- clean_tweets(tweets$text)
text[[1]]
text[[7]]

# function to classify individual tweets
classify <- function(words, pos.words, neg.words){
  # count number of positive and negative word matches
  pos.matches <- sum(words %in% pos.words)
  neg.matches <- sum(words %in% neg.words)
  return(pos.matches - neg.matches)
}

classify(text[[1]], pos.words, neg.words)
classify(text[[7]], pos.words, neg.words)

classifier <- function(text, pos.words, neg.words){
  # classifier
  scores <- unlist(lapply(text, classify, pos.words, neg.words))
  n <- length(scores)
  positive <- as.integer(length(which(scores>0))/n*100)
  negative <- as.integer(length(which(scores<0))/n*100)
  neutral <- 100 - positive - negative
  cat("The sentiment of the combined tweets is:", positive,"% positive,",
      negative,"% negative,", neutral,"% neutral","(out of", n, "tweets.)\n")
}

# applying classifier function
classifier(text, pos.words, neg.words)

# We now have the results classifying the entire collection of tweets.

################################################
### CREATING CHARTS ILLUSTRATING THE RESULTS ###
################################################

# This creates a stacked bar chart showing the individual sentiment counts for each
# Twitter account, based on a CSV file of the results.

install.packages("reshape2", "ggthemes")

sentiment_indiv <- read.csv("indiv_results.csv", header=T, sep=",", fill=T)

library(reshape2)
indiv2 <- melt(sentiment_indiv, id.var="Source", measured=c("Positive","Neutral","Negative"))

# This chart's colors are green (positive), gray (neutral), and red (negative).

library(ggplot2)
chart <- ggplot(indiv2, aes(x = Source, y = value, fill = variable)) + geom_bar(stat = "identity")
chart + labs(x = "'Hard news' accounts on Twitter", y = "% of tweets") + scale_fill_manual(values=c("#00CC66", "#B8B8B8", "#FF5C33")) + theme(axis.text.x=element_text(angle=340,hjust=.1,vjust=1))

